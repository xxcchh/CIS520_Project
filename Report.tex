\documentclass[]{article}
\usepackage{amsfonts,amssymb,amsmath}
%\documentstyle[12pt,amsfonts]{article}
%\documentstyle{article}
\usepackage{biblatex}

\setlength{\topmargin}{-.5in}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\textwidth}{6.5truein}
\setlength{\textheight}{8.5truein}
\setcounter{MaxMatrixCols}{16}
%\input ../basicmath/basicmathmac.tex
%
%\input ../adgeomcs/lamacb.tex
\input ./mac-new.tex
\input ./mathmac-v2.tex
%\input ../adgeomcs/mac.tex
%\input ../adgeomcs/mathmac.tex

\def\fseq#1#2{(#1_{#2})_{#2\geq 1}}
\def\fsseq#1#2#3{(#1_{#3(#2)})_{#2\geq 1}}
\def\qleq{\sqsubseteq}

%
\begin{document}

\title{CIS520 Report: ML Crackers}   % type title between braces
\author{Francine Leech, Ziyin Qu, Chen Xiang}         % type author(s) between braces
\date{December 12, 2016}    % type date between braces
\maketitle

\section{Introduction}


\section{Preliminary Methods}
- eveyrthing tyat didn't work
- why it didnt work 
- image - Ziyin
- PCA/GMM 
- Other ensemble methods methods


\section{Main Methods}

\subsection{Naive Bayes}
Accuracy

The second main

\subsection{GentleBoost}

The second main method we utilized was an ensemble method. We used GentleBoost, a weak learning that was built by MATLAB under the fitensemble function. The method combines many weak learners into one high quality ensemble predictor. We chose this ensemble methods over the others offered by MATLAB, because it is preforms well with binary classification trees with many predictors (ensemble citation). \\

The input of the model was the $word_train$ data. We used a 10-fold cross validation method to observed how the model preformed, specified the use 300 learners, and the type of learner as 'tree'. The average cross validation error was 0.21. The algorithm classified joy and sadness well. \\

The method could have improved if we increased the number of learners, however it would have taken a very long time to train because the data is large. Initially we tried the method with the default number of learners, 100 trees, and found that the cross validation accuracy only improved slightly. This slight improvement with triple number of learners reveals that the data has some intricacies or patterns that the ensemble method cannot learn.   


\subsection{Support Vector Machine}

Support vector machines (SVMs) proved to the most promising method to classify the data. We used the MATLAB function, fitcsvm, to train an SVM model for binary classification on the on the $word_train$ data.

We tried a simple SVM by specifying a linear kernel. The cross validation error was 0.80. After experimenting with a variety of kernels, we found that the linear preformed the best. fitsvm allows you to make an assumption about the fraction of outliers in the data. While we could have gone through the raw tweets and looked through the data, we decided to experiment with 5\%, 10\%, 20\%, and 30\%. The cross validation error 0.21 as we increased the outlier percentage. 

Lastly we tried to optimize our SVM by using MATLABs built in method to optimize a cross-validated SVM using Bayes Optimization (citation). The method originates from The Elements of Statistical Learning, Hastie, Tibshirani, and Friedman (2009). Paraphasing from the MATLAB documentation,  "the model begins with generating 10 base points for a "green" class, distributed as 2D independent normals with mean (1,0) and unite variance. It then generates 10 base points for a "class" that is also distributed as 2-D independent normals with mean (0,1) and unit variance. For each of the classes, it generate 100 random points by choosing a base point, b, of the respective color uniformly at random. It then generates an independent random point with 2-D normal distribution with mean b and variance I/5, where I is the 2-by-2 identity matrix. After 100 points for each of the colors has been generated, the point are classified using fitcsvm. The function bayesopt is used to optimize the parameters of the final SVM model with respect to cross validation."





<<<<<<< Updated upstream
\section{Ensemble Method}
Ensembl: Sentiment Analysis 1 + Methods \\
(1) First, we use vader package in python to do sentimental analysis on each word shown on topwords list. Vader is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.
The input is the every word in topwords list and the output is the probability of negative, positive and neutral emotion that this word may express. \\
The output often looks like this, when type "funny" we can see \{'compound': 0.4404, 'neg': 0.0, 'neu': 0.0, 'pos': 1.0\}, which means it is an extremely postive word. \\
(2) Second, we attach those scores to every word in topwords list. The advantage is that, we can use this preliminary method to choose the extremely positive and extremely negative sentences by assuming that there are no
extreme postive words shown in negative sentences and vise versa. 
=======
\section{Final Method}
Ensembl: Sentiment Analysis 1 + Methods
>>>>>>> Stashed changes

Ensembl: Sentiment Analysis 2 + Methods
dictionry wasn't as good \\
From the raw tweets we notice that there are some words begin with "#", which may represent the topic this sentence belongs to. It is useful since some specific topics always express similar emotions, like "#family" usually means positive.
So instead of anaylizing the sentiment of each word, we can use sentence as the input and in this way get the average emotion scores of each word.\\
(1) First, do sentimental analysis on each sentence in raw tweets. For all the words that appear in this sentence, attach the result score to this words. \\
(2) Second, for every word we can get the average emotion score based on all the sentences that they have shown.



%confusion matrix 


\section{Discussion}

\section{Works Cited}
\begin{thebibliography}{1}

%\bibitem{ensemble} Documentation. Ensemble Methods - MATLAB & Simulink. N.p., n.d. Web. 11 Dec. 2016. 

\end{thebibliography}

\end{document}
